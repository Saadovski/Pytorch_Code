{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import List_to_Pytorch_Dataset\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import CNN1D\n",
    "import Smaller_input_CNN\n",
    "import DeepCNN\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.1+cpu\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STD 180\n",
      "WAL 183\n",
      "JOG 183\n",
      "JUM 183\n",
      "STU 200\n",
      "STN 200\n",
      "SCH 200\n",
      "SIT 190\n",
      "CHU 114\n",
      "CSI 200\n",
      "CSO 197\n",
      "FOL 192\n",
      "FKL 192\n",
      "BSC 191\n",
      "SDL 192\n"
     ]
    }
   ],
   "source": [
    "dataset = List_to_Pytorch_Dataset.MobifallData(augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 2797\n",
      "Data sample size: 250\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"Data sample size: {len(dataset[0][0][0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu' #cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = { 0  : \"STD\",\n",
    "           1  : \"WAL\",\n",
    "           2  : \"JOG\",\n",
    "           3  : \"JUM\",\n",
    "           4  : \"STU\",\n",
    "           5  : \"STN\",\n",
    "           6  : \"SCH\",\n",
    "           7  : \"SIT\",\n",
    "           8  : \"CHU\",\n",
    "           9  : \"CSI\",\n",
    "           10 : \"CSO\",\n",
    "           11 : \"FOL\",\n",
    "           12 : \"FKL\",\n",
    "           13 : \"BSC\",\n",
    "           14 : \"SDL\"\n",
    "           #15 : \"PFF\"\n",
    "           }\n",
    "\n",
    "classification_map = {}\n",
    "classification_map[\"Falls\"] = {\"TP\" : 0,\n",
    "                                 \"FP\" : 0,\n",
    "                                 \"TN\" : 0,\n",
    "                                 \"FN\" : 0}\n",
    "for key, value in labels.items():\n",
    "    classification_map[value] = {\"TP\" : 0,\n",
    "                                 \"FP\" : 0,\n",
    "                                 \"TN\" : 0,\n",
    "                                 \"FN\" : 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch%10 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_fall(label):\n",
    "    return label in [11, 12, 13, 14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    valid_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            pred = model(X)\n",
    "\n",
    "            for key, value in labels.items():\n",
    "                for i in range(len(y)):\n",
    "                    if y[i] == key:\n",
    "                        if pred[i].argmax() == key:\n",
    "                            classification_map[value][\"TP\"] += 1\n",
    "                        else:\n",
    "                            classification_map[value][\"FN\"] += 1\n",
    "                    elif pred[i].argmax() == key:\n",
    "                        classification_map[value][\"FP\"] += 1\n",
    "                    else:\n",
    "                        classification_map[value][\"TN\"] += 1\n",
    "                        \n",
    "            #this is for fall/not-fall classification\n",
    "            for i in range(len(y)):\n",
    "                if is_fall(y[i]):\n",
    "                    if is_fall(pred[i].argmax()):\n",
    "                        classification_map[\"Falls\"][\"TP\"] += 1\n",
    "                    else:\n",
    "                        classification_map[\"Falls\"][\"FN\"] += 1\n",
    "                elif is_fall(pred[i].argmax()):\n",
    "                    classification_map[\"Falls\"][\"FP\"] += 1\n",
    "                else:\n",
    "                    classification_map[\"Falls\"][\"TN\"] += 1\n",
    "                    \n",
    "\n",
    "            #model loss and accuracy\n",
    "            valid_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    #model stats\n",
    "    valid_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Validation Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {valid_loss:>8f} \\n\\n\")\n",
    "    return 100*correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    model = DeepCNN.CNN().to(device)\n",
    "    model.train()\n",
    "\n",
    "    #this is for mobile use adaptation\n",
    "    backend = \"qnnpack\"\n",
    "    model.qconfig = torch.quantization.get_default_qat_qconfig(backend)\n",
    "    model = torch.quantization.prepare_qat(model, inplace=False)\n",
    "\n",
    "    learning_rate = 0.001\n",
    "    epochs = 40\n",
    "\n",
    "    # Initialize the loss function and optimizer\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    #creating datasets\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_set, test_set = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "    \n",
    "\n",
    "    #Dataloaders for the datasets\n",
    "    train_dataloader = DataLoader(train_set, batch_size = 64, shuffle = True)\n",
    "    test_dataloader = DataLoader(test_set, batch_size = 64, shuffle = True)\n",
    "    \n",
    "    model.train()\n",
    "    for i in range(epochs):\n",
    "        print(i)\n",
    "        train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    \n",
    "    model.eval()\n",
    "    model.to('cpu')\n",
    "\n",
    "    model = torch.quantization.convert(model.eval(), inplace=False)\n",
    "    model_accuracy = valid_loop(test_dataloader, model, loss_fn)\n",
    "\n",
    "    return model, model_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "loss: 2.702601  [    0/ 2237]\n",
      "loss: 2.712802  [  640/ 2237]\n",
      "loss: 2.703469  [ 1280/ 2237]\n",
      "loss: 2.703236  [ 1920/ 2237]\n",
      "1\n",
      "loss: 2.698845  [    0/ 2237]\n",
      "loss: 2.683990  [  640/ 2237]\n",
      "loss: 2.697668  [ 1280/ 2237]\n",
      "loss: 2.725873  [ 1920/ 2237]\n",
      "2\n",
      "loss: 2.664810  [    0/ 2237]\n",
      "loss: 2.717226  [  640/ 2237]\n",
      "loss: 2.846455  [ 1280/ 2237]\n",
      "loss: 2.924891  [ 1920/ 2237]\n",
      "3\n",
      "loss: 3.014465  [    0/ 2237]\n",
      "loss: 2.986933  [  640/ 2237]\n",
      "loss: 4.003813  [ 1280/ 2237]\n",
      "loss: 3.479744  [ 1920/ 2237]\n",
      "4\n",
      "loss: 4.332331  [    0/ 2237]\n",
      "loss: 6.253130  [  640/ 2237]\n",
      "loss: 8.683530  [ 1280/ 2237]\n",
      "loss: 9.185909  [ 1920/ 2237]\n",
      "5\n",
      "loss: 15.485758  [    0/ 2237]\n",
      "loss: 10.666433  [  640/ 2237]\n",
      "loss: 27.168232  [ 1280/ 2237]\n",
      "loss: 40.453499  [ 1920/ 2237]\n",
      "6\n",
      "loss: 57.084961  [    0/ 2237]\n",
      "loss: 104.667557  [  640/ 2237]\n",
      "loss: 132.851089  [ 1280/ 2237]\n",
      "loss: 293.307526  [ 1920/ 2237]\n",
      "7\n",
      "loss: 251.332382  [    0/ 2237]\n",
      "loss: 707.068359  [  640/ 2237]\n",
      "loss: 878.795959  [ 1280/ 2237]\n",
      "loss: 1230.186768  [ 1920/ 2237]\n",
      "8\n",
      "loss: 2115.288330  [    0/ 2237]\n",
      "loss: 3582.732666  [  640/ 2237]\n",
      "loss: 1619.597290  [ 1280/ 2237]\n",
      "loss: 4112.232422  [ 1920/ 2237]\n",
      "9\n",
      "loss: 7539.308594  [    0/ 2237]\n",
      "loss: 5162.171875  [  640/ 2237]\n",
      "loss: 9904.810547  [ 1280/ 2237]\n",
      "loss: 6534.226562  [ 1920/ 2237]\n",
      "10\n",
      "loss: 16405.605469  [    0/ 2237]\n",
      "loss: 23044.765625  [  640/ 2237]\n",
      "loss: 19478.634766  [ 1280/ 2237]\n",
      "loss: 39813.726562  [ 1920/ 2237]\n",
      "11\n",
      "loss: 20699.623047  [    0/ 2237]\n",
      "loss: 39486.828125  [  640/ 2237]\n",
      "loss: 62426.425781  [ 1280/ 2237]\n",
      "loss: 82461.195312  [ 1920/ 2237]\n",
      "12\n",
      "loss: 86156.046875  [    0/ 2237]\n",
      "loss: 90910.851562  [  640/ 2237]\n",
      "loss: 208738.031250  [ 1280/ 2237]\n",
      "loss: 116657.875000  [ 1920/ 2237]\n",
      "13\n",
      "loss: 332237.031250  [    0/ 2237]\n",
      "loss: 286238.562500  [  640/ 2237]\n",
      "loss: 404826.375000  [ 1280/ 2237]\n",
      "loss: 517463.625000  [ 1920/ 2237]\n",
      "14\n",
      "loss: 636950.000000  [    0/ 2237]\n",
      "loss: 874754.812500  [  640/ 2237]\n",
      "loss: 1187295.875000  [ 1280/ 2237]\n",
      "loss: 543648.375000  [ 1920/ 2237]\n",
      "15\n",
      "loss: 949189.187500  [    0/ 2237]\n",
      "loss: 1325650.250000  [  640/ 2237]\n",
      "loss: 2181182.250000  [ 1280/ 2237]\n",
      "loss: 1781706.625000  [ 1920/ 2237]\n",
      "16\n",
      "loss: 2459482.750000  [    0/ 2237]\n",
      "loss: 3281708.000000  [  640/ 2237]\n",
      "loss: 3235883.750000  [ 1280/ 2237]\n",
      "loss: 3444597.000000  [ 1920/ 2237]\n",
      "17\n",
      "loss: 5637567.000000  [    0/ 2237]\n",
      "loss: 6680422.000000  [  640/ 2237]\n",
      "loss: 10003728.000000  [ 1280/ 2237]\n",
      "loss: 4131778.500000  [ 1920/ 2237]\n",
      "18\n",
      "loss: 804534.437500  [    0/ 2237]\n",
      "loss: 943509.437500  [  640/ 2237]\n",
      "loss: 2276951.500000  [ 1280/ 2237]\n",
      "loss: 2.639057  [ 1920/ 2237]\n",
      "19\n",
      "loss: 4624972.000000  [    0/ 2237]\n",
      "loss: 5667017.000000  [  640/ 2237]\n",
      "loss: 4614555.500000  [ 1280/ 2237]\n",
      "loss: 2801989.250000  [ 1920/ 2237]\n",
      "20\n",
      "loss: 12319913.000000  [    0/ 2237]\n",
      "loss: 7405414.000000  [  640/ 2237]\n",
      "loss: 13262724.000000  [ 1280/ 2237]\n",
      "loss: 5243399.500000  [ 1920/ 2237]\n",
      "21\n",
      "loss: 17098472.000000  [    0/ 2237]\n",
      "loss: 26861288.000000  [  640/ 2237]\n",
      "loss: 15724876.000000  [ 1280/ 2237]\n",
      "loss: 9153883.000000  [ 1920/ 2237]\n",
      "22\n",
      "loss: 19715988.000000  [    0/ 2237]\n",
      "loss: 11387930.000000  [  640/ 2237]\n",
      "loss: 2.639057  [ 1280/ 2237]\n",
      "loss: 2.639057  [ 1920/ 2237]\n",
      "23\n",
      "loss: 66409848.000000  [    0/ 2237]\n",
      "loss: 58301376.000000  [  640/ 2237]\n",
      "loss: 22732160.000000  [ 1280/ 2237]\n",
      "loss: 159381728.000000  [ 1920/ 2237]\n",
      "24\n",
      "loss: 86096224.000000  [    0/ 2237]\n",
      "loss: 133727264.000000  [  640/ 2237]\n",
      "loss: 77737744.000000  [ 1280/ 2237]\n",
      "loss: 90140928.000000  [ 1920/ 2237]\n",
      "25\n",
      "loss: 145544576.000000  [    0/ 2237]\n",
      "loss: 112177424.000000  [  640/ 2237]\n",
      "loss: 258676192.000000  [ 1280/ 2237]\n",
      "loss: 74365728.000000  [ 1920/ 2237]\n",
      "26\n",
      "loss: 318747520.000000  [    0/ 2237]\n",
      "loss: 91304680.000000  [  640/ 2237]\n",
      "loss: 104308160.000000  [ 1280/ 2237]\n",
      "loss: 237653952.000000  [ 1920/ 2237]\n",
      "27\n",
      "loss: 380014752.000000  [    0/ 2237]\n",
      "loss: 2.639057  [  640/ 2237]\n",
      "loss: 162190240.000000  [ 1280/ 2237]\n",
      "loss: 365242720.000000  [ 1920/ 2237]\n",
      "28\n",
      "loss: 193560480.000000  [    0/ 2237]\n",
      "loss: 432492416.000000  [  640/ 2237]\n",
      "loss: 1920315648.000000  [ 1280/ 2237]\n",
      "loss: 265743984.000000  [ 1920/ 2237]\n",
      "29\n",
      "loss: 558859648.000000  [    0/ 2237]\n",
      "loss: 617393856.000000  [  640/ 2237]\n",
      "loss: 340672128.000000  [ 1280/ 2237]\n",
      "loss: 375666688.000000  [ 1920/ 2237]\n",
      "30\n",
      "loss: 788790016.000000  [    0/ 2237]\n",
      "loss: 2.639057  [  640/ 2237]\n",
      "loss: 956857216.000000  [ 1280/ 2237]\n",
      "loss: 1053265792.000000  [ 1920/ 2237]\n",
      "31\n",
      "loss: 552500992.000000  [    0/ 2237]\n",
      "loss: 608050944.000000  [  640/ 2237]\n",
      "loss: 2.639057  [ 1280/ 2237]\n",
      "loss: 2208016896.000000  [ 1920/ 2237]\n",
      "32\n",
      "loss: 3087420672.000000  [    0/ 2237]\n",
      "loss: 848672000.000000  [  640/ 2237]\n",
      "loss: 932786368.000000  [ 1280/ 2237]\n",
      "loss: 4099604480.000000  [ 1920/ 2237]\n",
      "33\n",
      "loss: 1074136320.000000  [    0/ 2237]\n",
      "loss: 3537869568.000000  [  640/ 2237]\n",
      "loss: 1293965824.000000  [ 1280/ 2237]\n",
      "loss: 4256316672.000000  [ 1920/ 2237]\n",
      "34\n",
      "loss: 2970269696.000000  [    0/ 2237]\n",
      "loss: 3289180928.000000  [  640/ 2237]\n",
      "loss: 3590617344.000000  [ 1280/ 2237]\n",
      "loss: 3969617408.000000  [ 1920/ 2237]\n",
      "35\n",
      "loss: 4185408000.000000  [    0/ 2237]\n",
      "loss: 4660568064.000000  [  640/ 2237]\n",
      "loss: 5193457664.000000  [ 1280/ 2237]\n",
      "loss: 8680417280.000000  [ 1920/ 2237]\n",
      "36\n",
      "loss: 3053747712.000000  [    0/ 2237]\n",
      "loss: 13606949888.000000  [  640/ 2237]\n",
      "loss: 15141517312.000000  [ 1280/ 2237]\n",
      "loss: 4210470144.000000  [ 1920/ 2237]\n",
      "37\n",
      "loss: 8878470144.000000  [    0/ 2237]\n",
      "loss: 19717560320.000000  [  640/ 2237]\n",
      "loss: 10932000768.000000  [ 1280/ 2237]\n",
      "loss: 24207001600.000000  [ 1920/ 2237]\n",
      "38\n",
      "loss: 2.639057  [    0/ 2237]\n",
      "loss: 21107601408.000000  [  640/ 2237]\n",
      "loss: 2.639057  [ 1280/ 2237]\n",
      "loss: 17137823744.000000  [ 1920/ 2237]\n",
      "39\n",
      "loss: 18026708992.000000  [    0/ 2237]\n",
      "loss: 29887283200.000000  [  640/ 2237]\n",
      "loss: 32943532032.000000  [ 1280/ 2237]\n",
      "loss: 60345372672.000000  [ 1920/ 2237]\n",
      "Validation Error: \n",
      " Accuracy: 6.1%, Avg loss: 46387221845.333336 \n",
      "\n",
      "\n",
      "0\n",
      "loss: 2.702146  [    0/ 2237]\n",
      "loss: 2.712017  [  640/ 2237]\n",
      "loss: 2.700600  [ 1280/ 2237]\n",
      "loss: 2.698591  [ 1920/ 2237]\n",
      "1\n",
      "loss: 2.717887  [    0/ 2237]\n",
      "loss: 2.706294  [  640/ 2237]\n",
      "loss: 2.702823  [ 1280/ 2237]\n",
      "loss: 2.703973  [ 1920/ 2237]\n",
      "2\n",
      "loss: 2.698252  [    0/ 2237]\n",
      "loss: 2.651927  [  640/ 2237]\n",
      "loss: 2.621319  [ 1280/ 2237]\n",
      "loss: 2.830596  [ 1920/ 2237]\n",
      "3\n",
      "loss: 3.012908  [    0/ 2237]\n",
      "loss: 4.920199  [  640/ 2237]\n",
      "loss: 6.073792  [ 1280/ 2237]\n",
      "loss: 11.611390  [ 1920/ 2237]\n",
      "4\n",
      "loss: 11.667948  [    0/ 2237]\n",
      "loss: 4.116044  [  640/ 2237]\n",
      "loss: 3.982723  [ 1280/ 2237]\n",
      "loss: 2.949183  [ 1920/ 2237]\n",
      "5\n",
      "loss: 2.921627  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "6\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "7\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "8\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "9\n",
      "loss: 2.706972  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "10\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "11\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "12\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "13\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "14\n",
      "loss: 542.976196  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "15\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "16\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "17\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "18\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "19\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "20\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "21\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "22\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "23\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "24\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "25\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "26\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "27\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "28\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "29\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "30\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "31\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.690884  [ 1280/ 2237]\n",
      "loss: 20886898.000000  [ 1920/ 2237]\n",
      "32\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "33\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 22440192.000000  [ 1920/ 2237]\n",
      "34\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "35\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.639057  [ 1920/ 2237]\n",
      "36\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "37\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.699717  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "38\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "39\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "Validation Error: \n",
      " Accuracy: 7.5%, Avg loss: 2.673134 \n",
      "\n",
      "\n",
      "0\n",
      "loss: 2.710113  [    0/ 2237]\n",
      "loss: 2.708610  [  640/ 2237]\n",
      "loss: 2.718060  [ 1280/ 2237]\n",
      "loss: 2.698527  [ 1920/ 2237]\n",
      "1\n",
      "loss: 2.718159  [    0/ 2237]\n",
      "loss: 2.714776  [  640/ 2237]\n",
      "loss: 2.696998  [ 1280/ 2237]\n",
      "loss: 2.705065  [ 1920/ 2237]\n",
      "2\n",
      "loss: 2.743248  [    0/ 2237]\n",
      "loss: 2.816901  [  640/ 2237]\n",
      "loss: 2.893620  [ 1280/ 2237]\n",
      "loss: 3.367343  [ 1920/ 2237]\n",
      "3\n",
      "loss: 3.496761  [    0/ 2237]\n",
      "loss: 3.588191  [  640/ 2237]\n",
      "loss: 3.618978  [ 1280/ 2237]\n",
      "loss: 4.416032  [ 1920/ 2237]\n",
      "4\n",
      "loss: 4.905481  [    0/ 2237]\n",
      "loss: 6.437219  [  640/ 2237]\n",
      "loss: 9.124191  [ 1280/ 2237]\n",
      "loss: 13.734624  [ 1920/ 2237]\n",
      "5\n",
      "loss: 15.270288  [    0/ 2237]\n",
      "loss: 17.685181  [  640/ 2237]\n",
      "loss: 27.144085  [ 1280/ 2237]\n",
      "loss: 72.501335  [ 1920/ 2237]\n",
      "6\n",
      "loss: 79.890717  [    0/ 2237]\n",
      "loss: 31.812090  [  640/ 2237]\n",
      "loss: 153.169846  [ 1280/ 2237]\n",
      "loss: 156.548416  [ 1920/ 2237]\n",
      "7\n",
      "loss: 258.031555  [    0/ 2237]\n",
      "loss: 469.848572  [  640/ 2237]\n",
      "loss: 688.158752  [ 1280/ 2237]\n",
      "loss: 187.694687  [ 1920/ 2237]\n",
      "8\n",
      "loss: 326.082886  [    0/ 2237]\n",
      "loss: 740.063354  [  640/ 2237]\n",
      "loss: 1422.897705  [ 1280/ 2237]\n",
      "loss: 2518.155518  [ 1920/ 2237]\n",
      "9\n",
      "loss: 986.647644  [    0/ 2237]\n",
      "loss: 4511.254395  [  640/ 2237]\n",
      "loss: 2473.432617  [ 1280/ 2237]\n",
      "loss: 6743.650391  [ 1920/ 2237]\n",
      "10\n",
      "loss: 6877.275391  [    0/ 2237]\n",
      "loss: 10621.133789  [  640/ 2237]\n",
      "loss: 16053.256836  [ 1280/ 2237]\n",
      "loss: 16628.623047  [ 1920/ 2237]\n",
      "11\n",
      "loss: 21849.517578  [    0/ 2237]\n",
      "loss: 39408.375000  [  640/ 2237]\n",
      "loss: 37197.070312  [ 1280/ 2237]\n",
      "loss: 35789.207031  [ 1920/ 2237]\n",
      "12\n",
      "loss: 47082.417969  [    0/ 2237]\n",
      "loss: 58971.500000  [  640/ 2237]\n",
      "loss: 124639.765625  [ 1280/ 2237]\n",
      "loss: 130107.648438  [ 1920/ 2237]\n",
      "13\n",
      "loss: 102248.812500  [    0/ 2237]\n",
      "loss: 110808.085938  [  640/ 2237]\n",
      "loss: 186969.000000  [ 1280/ 2237]\n",
      "loss: 147763.421875  [ 1920/ 2237]\n",
      "14\n",
      "loss: 132891.015625  [    0/ 2237]\n",
      "loss: 167673.453125  [  640/ 2237]\n",
      "loss: 368968.937500  [ 1280/ 2237]\n",
      "loss: 198241.078125  [ 1920/ 2237]\n",
      "15\n",
      "loss: 738649.375000  [    0/ 2237]\n",
      "loss: 183975.609375  [  640/ 2237]\n",
      "loss: 342117.062500  [ 1280/ 2237]\n",
      "loss: 1687899.250000  [ 1920/ 2237]\n",
      "16\n",
      "loss: 623226.250000  [    0/ 2237]\n",
      "loss: 952804.437500  [  640/ 2237]\n",
      "loss: 927540.437500  [ 1280/ 2237]\n",
      "loss: 1964573.000000  [ 1920/ 2237]\n",
      "17\n",
      "loss: 1540498.875000  [    0/ 2237]\n",
      "loss: 2218427.250000  [  640/ 2237]\n",
      "loss: 3529405.500000  [ 1280/ 2237]\n",
      "loss: 3662708.750000  [ 1920/ 2237]\n",
      "18\n",
      "loss: 5116411.000000  [    0/ 2237]\n",
      "loss: 4675194.500000  [  640/ 2237]\n",
      "loss: 3898117.250000  [ 1280/ 2237]\n",
      "loss: 7256370.500000  [ 1920/ 2237]\n",
      "19\n",
      "loss: 6868512.000000  [    0/ 2237]\n",
      "loss: 8064199.500000  [  640/ 2237]\n",
      "loss: 6806932.000000  [ 1280/ 2237]\n",
      "loss: 16167688.000000  [ 1920/ 2237]\n",
      "20\n",
      "loss: 7058636.500000  [    0/ 2237]\n",
      "loss: 14735219.000000  [  640/ 2237]\n",
      "loss: 15068636.000000  [ 1280/ 2237]\n",
      "loss: 32939064.000000  [ 1920/ 2237]\n",
      "21\n",
      "loss: 6531674.500000  [    0/ 2237]\n",
      "loss: 31014098.000000  [  640/ 2237]\n",
      "loss: 45859428.000000  [ 1280/ 2237]\n",
      "loss: 27029408.000000  [ 1920/ 2237]\n",
      "22\n",
      "loss: 35160660.000000  [    0/ 2237]\n",
      "loss: 48045280.000000  [  640/ 2237]\n",
      "loss: 56269440.000000  [ 1280/ 2237]\n",
      "loss: 65747424.000000  [ 1920/ 2237]\n",
      "23\n",
      "loss: 50735644.000000  [    0/ 2237]\n",
      "loss: 82644816.000000  [  640/ 2237]\n",
      "loss: 95684368.000000  [ 1280/ 2237]\n",
      "loss: 141864992.000000  [ 1920/ 2237]\n",
      "24\n",
      "loss: 101429680.000000  [    0/ 2237]\n",
      "loss: 109853936.000000  [  640/ 2237]\n",
      "loss: 21725392.000000  [ 1280/ 2237]\n",
      "loss: 50895556.000000  [ 1920/ 2237]\n",
      "25\n",
      "loss: 121637728.000000  [    0/ 2237]\n",
      "loss: 66823740.000000  [  640/ 2237]\n",
      "loss: 81671480.000000  [ 1280/ 2237]\n",
      "loss: 199700064.000000  [ 1920/ 2237]\n",
      "26\n",
      "loss: 220599584.000000  [    0/ 2237]\n",
      "loss: 134120024.000000  [  640/ 2237]\n",
      "loss: 324260800.000000  [ 1280/ 2237]\n",
      "loss: 389185984.000000  [ 1920/ 2237]\n",
      "27\n",
      "loss: 318808384.000000  [    0/ 2237]\n",
      "loss: 252172608.000000  [  640/ 2237]\n",
      "loss: 296633280.000000  [ 1280/ 2237]\n",
      "loss: 693029120.000000  [ 1920/ 2237]\n",
      "28\n",
      "loss: 747076224.000000  [    0/ 2237]\n",
      "loss: 431849696.000000  [  640/ 2237]\n",
      "loss: 247995104.000000  [ 1280/ 2237]\n",
      "loss: 566088192.000000  [ 1920/ 2237]\n",
      "29\n",
      "loss: 905526720.000000  [    0/ 2237]\n",
      "loss: 341931360.000000  [  640/ 2237]\n",
      "loss: 770798976.000000  [ 1280/ 2237]\n",
      "loss: 1296540160.000000  [ 1920/ 2237]\n",
      "30\n",
      "loss: 456824896.000000  [    0/ 2237]\n",
      "loss: 2034414336.000000  [  640/ 2237]\n",
      "loss: 3944778752.000000  [ 1280/ 2237]\n",
      "loss: 2589391616.000000  [ 1920/ 2237]\n",
      "31\n",
      "loss: 2097769728.000000  [    0/ 2237]\n",
      "loss: 1640493568.000000  [  640/ 2237]\n",
      "loss: 3852358144.000000  [ 1280/ 2237]\n",
      "loss: 3382768896.000000  [ 1920/ 2237]\n",
      "32\n",
      "loss: 2436166144.000000  [    0/ 2237]\n",
      "loss: 2833211392.000000  [  640/ 2237]\n",
      "loss: 8187968000.000000  [ 1280/ 2237]\n",
      "loss: 7529645056.000000  [ 1920/ 2237]\n",
      "33\n",
      "loss: 6041917440.000000  [    0/ 2237]\n",
      "loss: 9186846720.000000  [  640/ 2237]\n",
      "loss: 5214600704.000000  [ 1280/ 2237]\n",
      "loss: 17685114880.000000  [ 1920/ 2237]\n",
      "34\n",
      "loss: 12516675584.000000  [    0/ 2237]\n",
      "loss: 14056421376.000000  [  640/ 2237]\n",
      "loss: 19667542016.000000  [ 1280/ 2237]\n",
      "loss: 4383914496.000000  [ 1920/ 2237]\n",
      "35\n",
      "loss: 23109214208.000000  [    0/ 2237]\n",
      "loss: 10245201920.000000  [  640/ 2237]\n",
      "loss: 22628554752.000000  [ 1280/ 2237]\n",
      "loss: 18688233472.000000  [ 1920/ 2237]\n",
      "36\n",
      "loss: 19586129920.000000  [    0/ 2237]\n",
      "loss: 21455593472.000000  [  640/ 2237]\n",
      "loss: 2.630952  [ 1280/ 2237]\n",
      "loss: 33955848192.000000  [ 1920/ 2237]\n",
      "37\n",
      "loss: 35334275072.000000  [    0/ 2237]\n",
      "loss: 19080646656.000000  [  640/ 2237]\n",
      "loss: 20561463296.000000  [ 1280/ 2237]\n",
      "loss: 66392395776.000000  [ 1920/ 2237]\n",
      "38\n",
      "loss: 34431803392.000000  [    0/ 2237]\n",
      "loss: 36999843840.000000  [  640/ 2237]\n",
      "loss: 13241667584.000000  [ 1280/ 2237]\n",
      "loss: 28534923264.000000  [ 1920/ 2237]\n",
      "39\n",
      "loss: 29653297152.000000  [    0/ 2237]\n",
      "loss: 2.639057  [  640/ 2237]\n",
      "loss: 2.639057  [ 1280/ 2237]\n",
      "loss: 56675090432.000000  [ 1920/ 2237]\n",
      "Validation Error: \n",
      " Accuracy: 6.2%, Avg loss: 33994310997.333332 \n",
      "\n",
      "\n",
      "0\n",
      "loss: 2.710642  [    0/ 2237]\n",
      "loss: 2.707544  [  640/ 2237]\n",
      "loss: 2.706869  [ 1280/ 2237]\n",
      "loss: 2.701694  [ 1920/ 2237]\n",
      "1\n",
      "loss: 2.700642  [    0/ 2237]\n",
      "loss: 2.714151  [  640/ 2237]\n",
      "loss: 2.708783  [ 1280/ 2237]\n",
      "loss: 2.694366  [ 1920/ 2237]\n",
      "2\n",
      "loss: 2.766997  [    0/ 2237]\n",
      "loss: 2.815957  [  640/ 2237]\n",
      "loss: 3.115314  [ 1280/ 2237]\n",
      "loss: 3.294438  [ 1920/ 2237]\n",
      "3\n",
      "loss: 3.381304  [    0/ 2237]\n",
      "loss: 4.534641  [  640/ 2237]\n",
      "loss: 5.024761  [ 1280/ 2237]\n",
      "loss: 5.897473  [ 1920/ 2237]\n",
      "4\n",
      "loss: 8.369082  [    0/ 2237]\n",
      "loss: 7.668425  [  640/ 2237]\n",
      "loss: 9.657273  [ 1280/ 2237]\n",
      "loss: 16.020618  [ 1920/ 2237]\n",
      "5\n",
      "loss: 21.476149  [    0/ 2237]\n",
      "loss: 34.676762  [  640/ 2237]\n",
      "loss: 31.150061  [ 1280/ 2237]\n",
      "loss: 41.777233  [ 1920/ 2237]\n",
      "6\n",
      "loss: 13.931379  [    0/ 2237]\n",
      "loss: 29.954197  [  640/ 2237]\n",
      "loss: 43.454769  [ 1280/ 2237]\n",
      "loss: 40.573765  [ 1920/ 2237]\n",
      "7\n",
      "loss: 48.144005  [    0/ 2237]\n",
      "loss: 103.354881  [  640/ 2237]\n",
      "loss: 70.109169  [ 1280/ 2237]\n",
      "loss: 79.777542  [ 1920/ 2237]\n",
      "8\n",
      "loss: 225.258011  [    0/ 2237]\n",
      "loss: 439.056396  [  640/ 2237]\n",
      "loss: 368.762482  [ 1280/ 2237]\n",
      "loss: 875.356689  [ 1920/ 2237]\n",
      "9\n",
      "loss: 1325.802734  [    0/ 2237]\n",
      "loss: 961.043579  [  640/ 2237]\n",
      "loss: 820.541077  [ 1280/ 2237]\n",
      "loss: 764.082275  [ 1920/ 2237]\n",
      "10\n",
      "loss: 1789.615845  [    0/ 2237]\n",
      "loss: 6671.909668  [  640/ 2237]\n",
      "loss: 3236.099121  [ 1280/ 2237]\n",
      "loss: 8476.506836  [ 1920/ 2237]\n",
      "11\n",
      "loss: 8448.380859  [    0/ 2237]\n",
      "loss: 10830.321289  [  640/ 2237]\n",
      "loss: 9765.201172  [ 1280/ 2237]\n",
      "loss: 26714.966797  [ 1920/ 2237]\n",
      "12\n",
      "loss: 5391.085449  [    0/ 2237]\n",
      "loss: 19695.552734  [  640/ 2237]\n",
      "loss: 27622.779297  [ 1280/ 2237]\n",
      "loss: 32747.068359  [ 1920/ 2237]\n",
      "13\n",
      "loss: 30427.726562  [    0/ 2237]\n",
      "loss: 59385.019531  [  640/ 2237]\n",
      "loss: 34836.421875  [ 1280/ 2237]\n",
      "loss: 41231.039062  [ 1920/ 2237]\n",
      "14\n",
      "loss: 72106.476562  [    0/ 2237]\n",
      "loss: 86989.632812  [  640/ 2237]\n",
      "loss: 26562.314453  [ 1280/ 2237]\n",
      "loss: 147342.703125  [ 1920/ 2237]\n",
      "15\n",
      "loss: 127608.164062  [    0/ 2237]\n",
      "loss: 227097.671875  [  640/ 2237]\n",
      "loss: 170694.296875  [ 1280/ 2237]\n",
      "loss: 322500.843750  [ 1920/ 2237]\n",
      "16\n",
      "loss: 201300.015625  [    0/ 2237]\n",
      "loss: 458592.343750  [  640/ 2237]\n",
      "loss: 840299.437500  [ 1280/ 2237]\n",
      "loss: 902092.437500  [ 1920/ 2237]\n",
      "17\n",
      "loss: 461732.343750  [    0/ 2237]\n",
      "loss: 818265.437500  [  640/ 2237]\n",
      "loss: 1474770.375000  [ 1280/ 2237]\n",
      "loss: 1296697.375000  [ 1920/ 2237]\n",
      "18\n",
      "loss: 1243420.750000  [    0/ 2237]\n",
      "loss: 1550057.125000  [  640/ 2237]\n",
      "loss: 1921387.500000  [ 1280/ 2237]\n",
      "loss: 2763005.000000  [ 1920/ 2237]\n",
      "19\n",
      "loss: 2185007.000000  [    0/ 2237]\n",
      "loss: 1597640.250000  [  640/ 2237]\n",
      "loss: 7729019.500000  [ 1280/ 2237]\n",
      "loss: 5415800.500000  [ 1920/ 2237]\n",
      "20\n",
      "loss: 8456675.000000  [    0/ 2237]\n",
      "loss: 6031349.000000  [  640/ 2237]\n",
      "loss: 9490860.000000  [ 1280/ 2237]\n",
      "loss: 8341269.500000  [ 1920/ 2237]\n",
      "21\n",
      "loss: 9003397.000000  [    0/ 2237]\n",
      "loss: 5097407.500000  [  640/ 2237]\n",
      "loss: 11475385.000000  [ 1280/ 2237]\n",
      "loss: 13142377.000000  [ 1920/ 2237]\n",
      "22\n",
      "loss: 18853496.000000  [    0/ 2237]\n",
      "loss: 19254306.000000  [  640/ 2237]\n",
      "loss: 19412684.000000  [ 1280/ 2237]\n",
      "loss: 30588942.000000  [ 1920/ 2237]\n",
      "23\n",
      "loss: 20807166.000000  [    0/ 2237]\n",
      "loss: 34523252.000000  [  640/ 2237]\n",
      "loss: 58435804.000000  [ 1280/ 2237]\n",
      "loss: 41500984.000000  [ 1920/ 2237]\n",
      "24\n",
      "loss: 7517966.000000  [    0/ 2237]\n",
      "loss: 62070916.000000  [  640/ 2237]\n",
      "loss: 20861952.000000  [ 1280/ 2237]\n",
      "loss: 36677508.000000  [ 1920/ 2237]\n",
      "25\n",
      "loss: 105745904.000000  [    0/ 2237]\n",
      "loss: 154117088.000000  [  640/ 2237]\n",
      "loss: 107436064.000000  [ 1280/ 2237]\n",
      "loss: 248942672.000000  [ 1920/ 2237]\n",
      "26\n",
      "loss: 111513080.000000  [    0/ 2237]\n",
      "loss: 205609440.000000  [  640/ 2237]\n",
      "loss: 265526624.000000  [ 1280/ 2237]\n",
      "loss: 202466064.000000  [ 1920/ 2237]\n",
      "27\n",
      "loss: 252453600.000000  [    0/ 2237]\n",
      "loss: 329060448.000000  [  640/ 2237]\n",
      "loss: 187868960.000000  [ 1280/ 2237]\n",
      "loss: 321076064.000000  [ 1920/ 2237]\n",
      "28\n",
      "loss: 342145184.000000  [    0/ 2237]\n",
      "loss: 258110048.000000  [  640/ 2237]\n",
      "loss: 290746976.000000  [ 1280/ 2237]\n",
      "loss: 814893632.000000  [ 1920/ 2237]\n",
      "29\n",
      "loss: 947404864.000000  [    0/ 2237]\n",
      "loss: 383473984.000000  [  640/ 2237]\n",
      "loss: 532087520.000000  [ 1280/ 2237]\n",
      "loss: 708864384.000000  [ 1920/ 2237]\n",
      "30\n",
      "loss: 749616512.000000  [    0/ 2237]\n",
      "loss: 985346112.000000  [  640/ 2237]\n",
      "loss: 1116846336.000000  [ 1280/ 2237]\n",
      "loss: 907157568.000000  [ 1920/ 2237]\n",
      "31\n",
      "loss: 968138880.000000  [    0/ 2237]\n",
      "loss: 1323180416.000000  [  640/ 2237]\n",
      "loss: 1506620672.000000  [ 1280/ 2237]\n",
      "loss: 1999386368.000000  [ 1920/ 2237]\n",
      "32\n",
      "loss: 1827054976.000000  [    0/ 2237]\n",
      "loss: 3109799424.000000  [  640/ 2237]\n",
      "loss: 3129517056.000000  [ 1280/ 2237]\n",
      "loss: 4423124992.000000  [ 1920/ 2237]\n",
      "33\n",
      "loss: 4698960896.000000  [    0/ 2237]\n",
      "loss: 2796096512.000000  [  640/ 2237]\n",
      "loss: 2976253440.000000  [ 1280/ 2237]\n",
      "loss: 4014289152.000000  [ 1920/ 2237]\n",
      "34\n",
      "loss: 4251836672.000000  [    0/ 2237]\n",
      "loss: 6346210304.000000  [  640/ 2237]\n",
      "loss: 5311172096.000000  [ 1280/ 2237]\n",
      "loss: 8866439168.000000  [ 1920/ 2237]\n",
      "35\n",
      "loss: 7269214720.000000  [    0/ 2237]\n",
      "loss: 10362845184.000000  [  640/ 2237]\n",
      "loss: 11457568768.000000  [ 1280/ 2237]\n",
      "loss: 8493527040.000000  [ 1920/ 2237]\n",
      "36\n",
      "loss: 10307889152.000000  [    0/ 2237]\n",
      "loss: 9680586752.000000  [  640/ 2237]\n",
      "loss: 10600472576.000000  [ 1280/ 2237]\n",
      "loss: 17394429952.000000  [ 1920/ 2237]\n",
      "37\n",
      "loss: 16178827264.000000  [    0/ 2237]\n",
      "loss: 4441061376.000000  [  640/ 2237]\n",
      "loss: 21955917824.000000  [ 1280/ 2237]\n",
      "loss: 10731067392.000000  [ 1920/ 2237]\n",
      "38\n",
      "loss: 30940860416.000000  [    0/ 2237]\n",
      "loss: 18534758400.000000  [  640/ 2237]\n",
      "loss: 6779021824.000000  [ 1280/ 2237]\n",
      "loss: 14865113088.000000  [ 1920/ 2237]\n",
      "39\n",
      "loss: 19445325824.000000  [    0/ 2237]\n",
      "loss: 29781309440.000000  [  640/ 2237]\n",
      "loss: 55761842176.000000  [ 1280/ 2237]\n",
      "loss: 35464998912.000000  [ 1920/ 2237]\n",
      "Validation Error: \n",
      " Accuracy: 6.1%, Avg loss: 35529696142.222221 \n",
      "\n",
      "\n",
      "0\n",
      "loss: 2.712527  [    0/ 2237]\n",
      "loss: 2.705112  [  640/ 2237]\n",
      "loss: 2.694934  [ 1280/ 2237]\n",
      "loss: 2.709771  [ 1920/ 2237]\n",
      "1\n",
      "loss: 2.697419  [    0/ 2237]\n",
      "loss: 2.718654  [  640/ 2237]\n",
      "loss: 2.705949  [ 1280/ 2237]\n",
      "loss: 2.716096  [ 1920/ 2237]\n",
      "2\n",
      "loss: 2.741681  [    0/ 2237]\n",
      "loss: 2.743525  [  640/ 2237]\n",
      "loss: 2.818050  [ 1280/ 2237]\n",
      "loss: 3.121790  [ 1920/ 2237]\n",
      "3\n",
      "loss: 3.371854  [    0/ 2237]\n",
      "loss: 2.705769  [  640/ 2237]\n",
      "loss: 2.705052  [ 1280/ 2237]\n",
      "loss: 2.708007  [ 1920/ 2237]\n",
      "4\n",
      "loss: 2.708099  [    0/ 2237]\n",
      "loss: 2.704339  [  640/ 2237]\n",
      "loss: 2.758147  [ 1280/ 2237]\n",
      "loss: 2.599777  [ 1920/ 2237]\n",
      "5\n",
      "loss: 2.558838  [    0/ 2237]\n",
      "loss: 2.409439  [  640/ 2237]\n",
      "loss: 2.174567  [ 1280/ 2237]\n",
      "loss: 1.913438  [ 1920/ 2237]\n",
      "6\n",
      "loss: 1.844006  [    0/ 2237]\n",
      "loss: 1.694021  [  640/ 2237]\n",
      "loss: 1.530273  [ 1280/ 2237]\n",
      "loss: 1.601512  [ 1920/ 2237]\n",
      "7\n",
      "loss: 1.659761  [    0/ 2237]\n",
      "loss: 1.540361  [  640/ 2237]\n",
      "loss: 1.479903  [ 1280/ 2237]\n",
      "loss: 1.531322  [ 1920/ 2237]\n",
      "8\n",
      "loss: 1.349954  [    0/ 2237]\n",
      "loss: 1.432820  [  640/ 2237]\n",
      "loss: 1.266149  [ 1280/ 2237]\n",
      "loss: 1.578280  [ 1920/ 2237]\n",
      "9\n",
      "loss: 1.323800  [    0/ 2237]\n",
      "loss: 1.504948  [  640/ 2237]\n",
      "loss: 1.374417  [ 1280/ 2237]\n",
      "loss: 1.291068  [ 1920/ 2237]\n",
      "10\n",
      "loss: 1.486353  [    0/ 2237]\n",
      "loss: 1.441307  [  640/ 2237]\n",
      "loss: 1.285150  [ 1280/ 2237]\n",
      "loss: 1.317169  [ 1920/ 2237]\n",
      "11\n",
      "loss: 1.467784  [    0/ 2237]\n",
      "loss: 1.410123  [  640/ 2237]\n",
      "loss: 1.705990  [ 1280/ 2237]\n",
      "loss: 1.664961  [ 1920/ 2237]\n",
      "12\n",
      "loss: 1.537321  [    0/ 2237]\n",
      "loss: 1.676840  [  640/ 2237]\n",
      "loss: 1.333222  [ 1280/ 2237]\n",
      "loss: 1.392395  [ 1920/ 2237]\n",
      "13\n",
      "loss: 1.244383  [    0/ 2237]\n",
      "loss: 1.380678  [  640/ 2237]\n",
      "loss: 1.172447  [ 1280/ 2237]\n",
      "loss: 1.492820  [ 1920/ 2237]\n",
      "14\n",
      "loss: 1.400617  [    0/ 2237]\n",
      "loss: 1.337196  [  640/ 2237]\n",
      "loss: 1.291013  [ 1280/ 2237]\n",
      "loss: 1.219598  [ 1920/ 2237]\n",
      "15\n",
      "loss: 0.903666  [    0/ 2237]\n",
      "loss: 1.322219  [  640/ 2237]\n",
      "loss: 0.882998  [ 1280/ 2237]\n",
      "loss: 1.120770  [ 1920/ 2237]\n",
      "16\n",
      "loss: 1.138323  [    0/ 2237]\n",
      "loss: 0.952737  [  640/ 2237]\n",
      "loss: 0.993013  [ 1280/ 2237]\n",
      "loss: 0.850209  [ 1920/ 2237]\n",
      "17\n",
      "loss: 0.927635  [    0/ 2237]\n",
      "loss: 0.976837  [  640/ 2237]\n",
      "loss: 0.972556  [ 1280/ 2237]\n",
      "loss: 0.958439  [ 1920/ 2237]\n",
      "18\n",
      "loss: 0.981665  [    0/ 2237]\n",
      "loss: 1.106988  [  640/ 2237]\n",
      "loss: 0.926664  [ 1280/ 2237]\n",
      "loss: 0.944305  [ 1920/ 2237]\n",
      "19\n",
      "loss: 0.950047  [    0/ 2237]\n",
      "loss: 1.223157  [  640/ 2237]\n",
      "loss: 1.047539  [ 1280/ 2237]\n",
      "loss: 0.825555  [ 1920/ 2237]\n",
      "20\n",
      "loss: 0.860735  [    0/ 2237]\n",
      "loss: 0.997198  [  640/ 2237]\n",
      "loss: 0.773701  [ 1280/ 2237]\n",
      "loss: 0.837648  [ 1920/ 2237]\n",
      "21\n",
      "loss: 0.742224  [    0/ 2237]\n",
      "loss: 1.077571  [  640/ 2237]\n",
      "loss: 1.093734  [ 1280/ 2237]\n",
      "loss: 0.599744  [ 1920/ 2237]\n",
      "22\n",
      "loss: 0.834393  [    0/ 2237]\n",
      "loss: 0.807554  [  640/ 2237]\n",
      "loss: 0.999893  [ 1280/ 2237]\n",
      "loss: 0.960963  [ 1920/ 2237]\n",
      "23\n",
      "loss: 0.895710  [    0/ 2237]\n",
      "loss: 0.569237  [  640/ 2237]\n",
      "loss: 0.700569  [ 1280/ 2237]\n",
      "loss: 0.574119  [ 1920/ 2237]\n",
      "24\n",
      "loss: 0.895642  [    0/ 2237]\n",
      "loss: 0.963939  [  640/ 2237]\n",
      "loss: 0.822301  [ 1280/ 2237]\n",
      "loss: 0.632134  [ 1920/ 2237]\n",
      "25\n",
      "loss: 0.477002  [    0/ 2237]\n",
      "loss: 0.987579  [  640/ 2237]\n",
      "loss: 1.011098  [ 1280/ 2237]\n",
      "loss: 0.940846  [ 1920/ 2237]\n",
      "26\n",
      "loss: 0.761701  [    0/ 2237]\n",
      "loss: 0.921011  [  640/ 2237]\n",
      "loss: 0.702189  [ 1280/ 2237]\n",
      "loss: 0.847290  [ 1920/ 2237]\n",
      "27\n",
      "loss: 0.769457  [    0/ 2237]\n",
      "loss: 0.831133  [  640/ 2237]\n",
      "loss: 0.574680  [ 1280/ 2237]\n",
      "loss: 0.505536  [ 1920/ 2237]\n",
      "28\n",
      "loss: 0.628087  [    0/ 2237]\n",
      "loss: 0.587694  [  640/ 2237]\n",
      "loss: 0.453449  [ 1280/ 2237]\n",
      "loss: 0.750498  [ 1920/ 2237]\n",
      "29\n",
      "loss: 0.926512  [    0/ 2237]\n",
      "loss: 0.487005  [  640/ 2237]\n",
      "loss: 0.507340  [ 1280/ 2237]\n",
      "loss: 0.483362  [ 1920/ 2237]\n",
      "30\n",
      "loss: 0.358251  [    0/ 2237]\n",
      "loss: 0.674734  [  640/ 2237]\n",
      "loss: 0.757174  [ 1280/ 2237]\n",
      "loss: 0.888965  [ 1920/ 2237]\n",
      "31\n",
      "loss: 0.796862  [    0/ 2237]\n",
      "loss: 1.028841  [  640/ 2237]\n",
      "loss: 0.852982  [ 1280/ 2237]\n",
      "loss: 0.600533  [ 1920/ 2237]\n",
      "32\n",
      "loss: 0.343700  [    0/ 2237]\n",
      "loss: 0.352838  [  640/ 2237]\n",
      "loss: 0.654266  [ 1280/ 2237]\n",
      "loss: 0.429449  [ 1920/ 2237]\n",
      "33\n",
      "loss: 0.498988  [    0/ 2237]\n",
      "loss: 0.683834  [  640/ 2237]\n",
      "loss: 0.663270  [ 1280/ 2237]\n",
      "loss: 0.563152  [ 1920/ 2237]\n",
      "34\n",
      "loss: 0.768079  [    0/ 2237]\n",
      "loss: 0.629261  [  640/ 2237]\n",
      "loss: 0.783214  [ 1280/ 2237]\n",
      "loss: 0.456624  [ 1920/ 2237]\n",
      "35\n",
      "loss: 0.692603  [    0/ 2237]\n",
      "loss: 0.645640  [  640/ 2237]\n",
      "loss: 0.732025  [ 1280/ 2237]\n",
      "loss: 0.674561  [ 1920/ 2237]\n",
      "36\n",
      "loss: 0.578563  [    0/ 2237]\n",
      "loss: 0.514319  [  640/ 2237]\n",
      "loss: 0.502245  [ 1280/ 2237]\n",
      "loss: 0.906695  [ 1920/ 2237]\n",
      "37\n",
      "loss: 0.968587  [    0/ 2237]\n",
      "loss: 1.379284  [  640/ 2237]\n",
      "loss: 0.700721  [ 1280/ 2237]\n",
      "loss: 0.636866  [ 1920/ 2237]\n",
      "38\n",
      "loss: 0.410726  [    0/ 2237]\n",
      "loss: 0.317020  [  640/ 2237]\n",
      "loss: 0.717935  [ 1280/ 2237]\n",
      "loss: 0.717284  [ 1920/ 2237]\n",
      "39\n",
      "loss: 0.511237  [    0/ 2237]\n",
      "loss: 0.580492  [  640/ 2237]\n",
      "loss: 0.572920  [ 1280/ 2237]\n",
      "loss: 0.442612  [ 1920/ 2237]\n",
      "Validation Error: \n",
      " Accuracy: 65.2%, Avg loss: 0.768410 \n",
      "\n",
      "\n",
      "0\n",
      "loss: 2.702673  [    0/ 2237]\n",
      "loss: 2.706338  [  640/ 2237]\n",
      "loss: 2.706173  [ 1280/ 2237]\n",
      "loss: 2.707707  [ 1920/ 2237]\n",
      "1\n",
      "loss: 2.711108  [    0/ 2237]\n",
      "loss: 2.715148  [  640/ 2237]\n",
      "loss: 2.702384  [ 1280/ 2237]\n",
      "loss: 2.667748  [ 1920/ 2237]\n",
      "2\n",
      "loss: 2.646722  [    0/ 2237]\n",
      "loss: 2.653379  [  640/ 2237]\n",
      "loss: 2.662419  [ 1280/ 2237]\n",
      "loss: 4.107224  [ 1920/ 2237]\n",
      "3\n",
      "loss: 4.575655  [    0/ 2237]\n",
      "loss: 6.088696  [  640/ 2237]\n",
      "loss: 5.803845  [ 1280/ 2237]\n",
      "loss: 6.437304  [ 1920/ 2237]\n",
      "4\n",
      "loss: 7.471592  [    0/ 2237]\n",
      "loss: 3.657188  [  640/ 2237]\n",
      "loss: 3.299210  [ 1280/ 2237]\n",
      "loss: 2.635761  [ 1920/ 2237]\n",
      "5\n",
      "loss: 2.629977  [    0/ 2237]\n",
      "loss: 2.616679  [  640/ 2237]\n",
      "loss: 2.615974  [ 1280/ 2237]\n",
      "loss: 2.799647  [ 1920/ 2237]\n",
      "6\n",
      "loss: 2.621823  [    0/ 2237]\n",
      "loss: 2.726257  [  640/ 2237]\n",
      "loss: 2.692398  [ 1280/ 2237]\n",
      "loss: 2.683455  [ 1920/ 2237]\n",
      "7\n",
      "loss: 3.702532  [    0/ 2237]\n",
      "loss: 2.681219  [  640/ 2237]\n",
      "loss: 2.842963  [ 1280/ 2237]\n",
      "loss: 2.691125  [ 1920/ 2237]\n",
      "8\n",
      "loss: 21.427164  [    0/ 2237]\n",
      "loss: 2.692398  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "9\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "10\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "11\n",
      "loss: 4.542612  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.692087  [ 1920/ 2237]\n",
      "12\n",
      "loss: 3.288774  [    0/ 2237]\n",
      "loss: 2.639299  [  640/ 2237]\n",
      "loss: 2.858641  [ 1280/ 2237]\n",
      "loss: 4.183046  [ 1920/ 2237]\n",
      "13\n",
      "loss: 2.647048  [    0/ 2237]\n",
      "loss: 2.501992  [  640/ 2237]\n",
      "loss: 2.607771  [ 1280/ 2237]\n",
      "loss: 2.735541  [ 1920/ 2237]\n",
      "14\n",
      "loss: 2.424228  [    0/ 2237]\n",
      "loss: 2.522185  [  640/ 2237]\n",
      "loss: 2.642539  [ 1280/ 2237]\n",
      "loss: 2.655998  [ 1920/ 2237]\n",
      "15\n",
      "loss: 3.089256  [    0/ 2237]\n",
      "loss: 2.582342  [  640/ 2237]\n",
      "loss: 2.663674  [ 1280/ 2237]\n",
      "loss: 2.496575  [ 1920/ 2237]\n",
      "16\n",
      "loss: 2.614157  [    0/ 2237]\n",
      "loss: 2.624904  [  640/ 2237]\n",
      "loss: 2.554462  [ 1280/ 2237]\n",
      "loss: 2.553366  [ 1920/ 2237]\n",
      "17\n",
      "loss: 2.414380  [    0/ 2237]\n",
      "loss: 2.363920  [  640/ 2237]\n",
      "loss: 2.271087  [ 1280/ 2237]\n",
      "loss: 2.225665  [ 1920/ 2237]\n",
      "18\n",
      "loss: 2.072963  [    0/ 2237]\n",
      "loss: 2.112803  [  640/ 2237]\n",
      "loss: 2.134782  [ 1280/ 2237]\n",
      "loss: 1.881103  [ 1920/ 2237]\n",
      "19\n",
      "loss: 2.249794  [    0/ 2237]\n",
      "loss: 1.765560  [  640/ 2237]\n",
      "loss: 1.988479  [ 1280/ 2237]\n",
      "loss: 1.755515  [ 1920/ 2237]\n",
      "20\n",
      "loss: 1.824413  [    0/ 2237]\n",
      "loss: 1.607809  [  640/ 2237]\n",
      "loss: 1.759094  [ 1280/ 2237]\n",
      "loss: 1.812610  [ 1920/ 2237]\n",
      "21\n",
      "loss: 1.970821  [    0/ 2237]\n",
      "loss: 1.757332  [  640/ 2237]\n",
      "loss: 1.958700  [ 1280/ 2237]\n",
      "loss: 1.779508  [ 1920/ 2237]\n",
      "22\n",
      "loss: 1.862078  [    0/ 2237]\n",
      "loss: 1.711584  [  640/ 2237]\n",
      "loss: 1.642301  [ 1280/ 2237]\n",
      "loss: 1.609725  [ 1920/ 2237]\n",
      "23\n",
      "loss: 1.689507  [    0/ 2237]\n",
      "loss: 1.599084  [  640/ 2237]\n",
      "loss: 1.538106  [ 1280/ 2237]\n",
      "loss: 1.756382  [ 1920/ 2237]\n",
      "24\n",
      "loss: 1.450846  [    0/ 2237]\n",
      "loss: 2.101336  [  640/ 2237]\n",
      "loss: 2.034507  [ 1280/ 2237]\n",
      "loss: 1.632508  [ 1920/ 2237]\n",
      "25\n",
      "loss: 1.679476  [    0/ 2237]\n",
      "loss: 1.456051  [  640/ 2237]\n",
      "loss: 1.746020  [ 1280/ 2237]\n",
      "loss: 1.398101  [ 1920/ 2237]\n",
      "26\n",
      "loss: 1.495931  [    0/ 2237]\n",
      "loss: 1.524835  [  640/ 2237]\n",
      "loss: 1.697520  [ 1280/ 2237]\n",
      "loss: 1.292446  [ 1920/ 2237]\n",
      "27\n",
      "loss: 1.710654  [    0/ 2237]\n",
      "loss: 1.517343  [  640/ 2237]\n",
      "loss: 1.496959  [ 1280/ 2237]\n",
      "loss: 1.397870  [ 1920/ 2237]\n",
      "28\n",
      "loss: 1.252818  [    0/ 2237]\n",
      "loss: 1.263035  [  640/ 2237]\n",
      "loss: 1.333383  [ 1280/ 2237]\n",
      "loss: 1.534355  [ 1920/ 2237]\n",
      "29\n",
      "loss: 1.273721  [    0/ 2237]\n",
      "loss: 1.385375  [  640/ 2237]\n",
      "loss: 1.316022  [ 1280/ 2237]\n",
      "loss: 1.224007  [ 1920/ 2237]\n",
      "30\n",
      "loss: 1.152051  [    0/ 2237]\n",
      "loss: 1.098929  [  640/ 2237]\n",
      "loss: 1.380316  [ 1280/ 2237]\n",
      "loss: 1.283996  [ 1920/ 2237]\n",
      "31\n",
      "loss: 1.204728  [    0/ 2237]\n",
      "loss: 1.212008  [  640/ 2237]\n",
      "loss: 1.200475  [ 1280/ 2237]\n",
      "loss: 1.088399  [ 1920/ 2237]\n",
      "32\n",
      "loss: 1.236985  [    0/ 2237]\n",
      "loss: 1.484137  [  640/ 2237]\n",
      "loss: 1.308976  [ 1280/ 2237]\n",
      "loss: 1.269679  [ 1920/ 2237]\n",
      "33\n",
      "loss: 1.175468  [    0/ 2237]\n",
      "loss: 1.075381  [  640/ 2237]\n",
      "loss: 1.072954  [ 1280/ 2237]\n",
      "loss: 1.201583  [ 1920/ 2237]\n",
      "34\n",
      "loss: 1.022074  [    0/ 2237]\n",
      "loss: 1.349788  [  640/ 2237]\n",
      "loss: 1.226026  [ 1280/ 2237]\n",
      "loss: 1.148422  [ 1920/ 2237]\n",
      "35\n",
      "loss: 1.169961  [    0/ 2237]\n",
      "loss: 1.071038  [  640/ 2237]\n",
      "loss: 1.000589  [ 1280/ 2237]\n",
      "loss: 1.278176  [ 1920/ 2237]\n",
      "36\n",
      "loss: 0.977369  [    0/ 2237]\n",
      "loss: 1.104962  [  640/ 2237]\n",
      "loss: 1.140018  [ 1280/ 2237]\n",
      "loss: 1.203556  [ 1920/ 2237]\n",
      "37\n",
      "loss: 1.107239  [    0/ 2237]\n",
      "loss: 1.148124  [  640/ 2237]\n",
      "loss: 1.126079  [ 1280/ 2237]\n",
      "loss: 1.148436  [ 1920/ 2237]\n",
      "38\n",
      "loss: 1.014047  [    0/ 2237]\n",
      "loss: 1.005859  [  640/ 2237]\n",
      "loss: 1.096840  [ 1280/ 2237]\n",
      "loss: 1.136876  [ 1920/ 2237]\n",
      "39\n",
      "loss: 0.953580  [    0/ 2237]\n",
      "loss: 0.947944  [  640/ 2237]\n",
      "loss: 0.854581  [ 1280/ 2237]\n",
      "loss: 1.197270  [ 1920/ 2237]\n",
      "Validation Error: \n",
      " Accuracy: 53.2%, Avg loss: 1.233174 \n",
      "\n",
      "\n",
      "0\n",
      "loss: 2.704614  [    0/ 2237]\n",
      "loss: 2.727019  [  640/ 2237]\n",
      "loss: 2.706461  [ 1280/ 2237]\n",
      "loss: 2.701761  [ 1920/ 2237]\n",
      "1\n",
      "loss: 2.726394  [    0/ 2237]\n",
      "loss: 2.715664  [  640/ 2237]\n",
      "loss: 2.694092  [ 1280/ 2237]\n",
      "loss: 2.735350  [ 1920/ 2237]\n",
      "2\n",
      "loss: 2.722221  [    0/ 2237]\n",
      "loss: 2.671792  [  640/ 2237]\n",
      "loss: 2.797895  [ 1280/ 2237]\n",
      "loss: 2.816084  [ 1920/ 2237]\n",
      "3\n",
      "loss: 2.933603  [    0/ 2237]\n",
      "loss: 3.371995  [  640/ 2237]\n",
      "loss: 3.402172  [ 1280/ 2237]\n",
      "loss: 4.316524  [ 1920/ 2237]\n",
      "4\n",
      "loss: 4.800952  [    0/ 2237]\n",
      "loss: 5.315799  [  640/ 2237]\n",
      "loss: 9.689273  [ 1280/ 2237]\n",
      "loss: 8.502767  [ 1920/ 2237]\n",
      "5\n",
      "loss: 15.513124  [    0/ 2237]\n",
      "loss: 29.508007  [  640/ 2237]\n",
      "loss: 27.359894  [ 1280/ 2237]\n",
      "loss: 54.834068  [ 1920/ 2237]\n",
      "6\n",
      "loss: 83.869637  [    0/ 2237]\n",
      "loss: 71.677765  [  640/ 2237]\n",
      "loss: 181.705765  [ 1280/ 2237]\n",
      "loss: 126.380142  [ 1920/ 2237]\n",
      "7\n",
      "loss: 59.606632  [    0/ 2237]\n",
      "loss: 5.942836  [  640/ 2237]\n",
      "loss: 11.687144  [ 1280/ 2237]\n",
      "loss: 8.295042  [ 1920/ 2237]\n",
      "8\n",
      "loss: 6.746769  [    0/ 2237]\n",
      "loss: 20.363710  [  640/ 2237]\n",
      "loss: 5.685771  [ 1280/ 2237]\n",
      "loss: 3.649050  [ 1920/ 2237]\n",
      "9\n",
      "loss: 25.521683  [    0/ 2237]\n",
      "loss: 2.706972  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 5.685072  [ 1920/ 2237]\n",
      "10\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "11\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "12\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "13\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.706972  [ 1920/ 2237]\n",
      "14\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "15\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "16\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "17\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "18\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "19\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "20\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.703738  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "21\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "22\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "23\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "24\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "25\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "26\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "27\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "28\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "29\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.706972  [ 1920/ 2237]\n",
      "30\n",
      "loss: 2.706972  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "31\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "32\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "33\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "34\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "35\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.705894  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "36\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "37\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "38\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "39\n",
      "loss: 2.708050  [    0/ 2237]\n",
      "loss: 2.708050  [  640/ 2237]\n",
      "loss: 2.708050  [ 1280/ 2237]\n",
      "loss: 2.708050  [ 1920/ 2237]\n",
      "Validation Error: \n",
      " Accuracy: 8.8%, Avg loss: 2.676644 \n",
      "\n",
      "\n",
      "0\n",
      "loss: 2.713587  [    0/ 2237]\n",
      "loss: 2.712060  [  640/ 2237]\n",
      "loss: 2.715011  [ 1280/ 2237]\n",
      "loss: 2.702206  [ 1920/ 2237]\n",
      "1\n",
      "loss: 2.718539  [    0/ 2237]\n",
      "loss: 2.701342  [  640/ 2237]\n",
      "loss: 2.686742  [ 1280/ 2237]\n",
      "loss: 2.696584  [ 1920/ 2237]\n",
      "2\n",
      "loss: 2.721025  [    0/ 2237]\n",
      "loss: 2.787660  [  640/ 2237]\n",
      "loss: 2.838228  [ 1280/ 2237]\n",
      "loss: 2.898864  [ 1920/ 2237]\n",
      "3\n",
      "loss: 3.314753  [    0/ 2237]\n",
      "loss: 3.112445  [  640/ 2237]\n",
      "loss: 3.789193  [ 1280/ 2237]\n",
      "loss: 4.007400  [ 1920/ 2237]\n",
      "4\n",
      "loss: 4.032901  [    0/ 2237]\n",
      "loss: 5.537325  [  640/ 2237]\n",
      "loss: 5.233433  [ 1280/ 2237]\n",
      "loss: 5.693444  [ 1920/ 2237]\n",
      "5\n",
      "loss: 6.302644  [    0/ 2237]\n",
      "loss: 9.466582  [  640/ 2237]\n",
      "loss: 9.896897  [ 1280/ 2237]\n",
      "loss: 16.368500  [ 1920/ 2237]\n",
      "6\n",
      "loss: 24.016335  [    0/ 2237]\n",
      "loss: 42.652946  [  640/ 2237]\n",
      "loss: 57.698372  [ 1280/ 2237]\n",
      "loss: 55.756546  [ 1920/ 2237]\n",
      "7\n",
      "loss: 84.376961  [    0/ 2237]\n",
      "loss: 48.735950  [  640/ 2237]\n",
      "loss: 52.515255  [ 1280/ 2237]\n",
      "loss: 177.277969  [ 1920/ 2237]\n",
      "8\n",
      "loss: 144.160416  [    0/ 2237]\n",
      "loss: 220.400818  [  640/ 2237]\n",
      "loss: 214.930099  [ 1280/ 2237]\n",
      "loss: 759.073792  [ 1920/ 2237]\n",
      "9\n",
      "loss: 692.692383  [    0/ 2237]\n",
      "loss: 943.424988  [  640/ 2237]\n",
      "loss: 1536.454834  [ 1280/ 2237]\n",
      "loss: 1074.666016  [ 1920/ 2237]\n",
      "10\n",
      "loss: 1771.213379  [    0/ 2237]\n",
      "loss: 2281.745605  [  640/ 2237]\n",
      "loss: 2903.215088  [ 1280/ 2237]\n",
      "loss: 3168.074463  [ 1920/ 2237]\n",
      "11\n",
      "loss: 3617.589111  [    0/ 2237]\n",
      "loss: 5191.503906  [  640/ 2237]\n",
      "loss: 4013.291748  [ 1280/ 2237]\n",
      "loss: 6851.819824  [ 1920/ 2237]\n",
      "12\n",
      "loss: 7894.901855  [    0/ 2237]\n",
      "loss: 15099.116211  [  640/ 2237]\n",
      "loss: 12254.857422  [ 1280/ 2237]\n",
      "loss: 16075.950195  [ 1920/ 2237]\n",
      "13\n",
      "loss: 20665.076172  [    0/ 2237]\n",
      "loss: 35817.996094  [  640/ 2237]\n",
      "loss: 50044.570312  [ 1280/ 2237]\n",
      "loss: 44405.261719  [ 1920/ 2237]\n",
      "14\n",
      "loss: 38975.765625  [    0/ 2237]\n",
      "loss: 98614.445312  [  640/ 2237]\n",
      "loss: 79488.171875  [ 1280/ 2237]\n",
      "loss: 109798.570312  [ 1920/ 2237]\n",
      "15\n",
      "loss: 97616.226562  [    0/ 2237]\n",
      "loss: 104931.414062  [  640/ 2237]\n",
      "loss: 262683.093750  [ 1280/ 2237]\n",
      "loss: 153810.484375  [ 1920/ 2237]\n",
      "16\n",
      "loss: 216403.906250  [    0/ 2237]\n",
      "loss: 515991.437500  [  640/ 2237]\n",
      "loss: 475043.406250  [ 1280/ 2237]\n",
      "loss: 439270.375000  [ 1920/ 2237]\n",
      "17\n",
      "loss: 216334.265625  [    0/ 2237]\n",
      "loss: 760407.562500  [  640/ 2237]\n",
      "loss: 889885.625000  [ 1280/ 2237]\n",
      "loss: 902242.937500  [ 1920/ 2237]\n",
      "18\n",
      "loss: 525506.625000  [    0/ 2237]\n",
      "loss: 1054210.250000  [  640/ 2237]\n",
      "loss: 1543112.625000  [ 1280/ 2237]\n",
      "loss: 1687733.750000  [ 1920/ 2237]\n",
      "19\n",
      "loss: 1566375.875000  [    0/ 2237]\n",
      "loss: 1684282.125000  [  640/ 2237]\n",
      "loss: 1976637.875000  [ 1280/ 2237]\n",
      "loss: 2952688.750000  [ 1920/ 2237]\n",
      "20\n",
      "loss: 3198568.500000  [    0/ 2237]\n",
      "loss: 2947852.000000  [  640/ 2237]\n",
      "loss: 3140366.750000  [ 1280/ 2237]\n",
      "loss: 5879637.500000  [ 1920/ 2237]\n",
      "21\n",
      "loss: 4369741.000000  [    0/ 2237]\n",
      "loss: 2780883.500000  [  640/ 2237]\n",
      "loss: 5933467.500000  [ 1280/ 2237]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-32e1073ee7af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mmodel_accuracies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m#for _ in range(5):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mmodel_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-28784a95c94a>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mtrain_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-c67fd29d6788>\u001b[0m in \u001b[0;36mtrain_loop\u001b[1;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;31m# Backpropagation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\souid\\miniconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 396\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\souid\\miniconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_accuracy = 0\n",
    "while model_accuracy < 90:\n",
    "    metric_list = []\n",
    "    model_accuracies = []\n",
    "    #for _ in range(5):\n",
    "    result = main()\n",
    "    model = result[0]\n",
    "    model_accuracy = result[1]\n",
    "    if model_accuracy < 90:\n",
    "        continue\n",
    "    model_accuracies.append(model_accuracy)\n",
    "    stats_map = {}\n",
    "    for key, value in classification_map.items():\n",
    "        stats_map[key] = {\n",
    "            \"Specificity\" : float(classification_map[key][\"TN\"]) / float(classification_map[key][\"TN\"] + classification_map[key][\"FP\"]),\n",
    "            \"Recall\" : float(classification_map[key][\"TP\"]) / float(classification_map[key][\"TP\"] + classification_map[key][\"FN\"]),\n",
    "            \"Precision\" : float(classification_map[key][\"TP\"]) / float(classification_map[key][\"TP\"] + classification_map[key][\"FP\"]),\n",
    "            \"Accuracy\" : float(classification_map[key][\"TP\"] + classification_map[key][\"TN\"]) / float(classification_map[key][\"TP\"] + classification_map[key][\"TN\"] + classification_map[key][\"FP\"] + classification_map[key][\"FN\"])\n",
    "        }\n",
    "        stats_map[key][\"F-score\"] = 2.0 / float((1.0 / float(stats_map[key][\"Precision\"])) + (1.0 / float(stats_map[key][\"Recall\"])))\n",
    "    metric_list.append(stats_map)\n",
    "    for key, value in classification_map.items():\n",
    "        classification_map[key] = {\"TP\" : 0,\n",
    "                                \"FP\" : 0,\n",
    "                                \"TN\" : 0,\n",
    "                                \"FN\" : 0}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specificity of Falls: 0.99226899006013\n",
      "Recall of Falls: 0.37921992789249426\n",
      "Precision of Falls: 0.9483606557377049\n",
      "F1-score of Falls: 0.5417934909857176\n",
      "\n",
      "Specificity of FOL: 0.9810162991371045\n",
      "Recall of FOL: 0.2948051948051948\n",
      "Precision of FOL: 0.5341176470588235\n",
      "F1-score of FOL: 0.3799163179916318\n",
      "\n",
      "Specificity of FKL: 0.9860415864458991\n",
      "Recall of FKL: 0.21921182266009853\n",
      "Precision of FKL: 0.5510835913312694\n",
      "F1-score of FKL: 0.31365638766519827\n",
      "\n",
      "Specificity of BSC: 0.99713384924047\n",
      "Recall of BSC: 0.2755798090040928\n",
      "Precision of BSC: 0.8706896551724138\n",
      "F1-score of BSC: 0.4186528497409327\n",
      "\n",
      "Specificity of SDL: 0.9928325688073395\n",
      "Recall of SDL: 0.22418478260869565\n",
      "Precision of SDL: 0.6875\n",
      "F1-score of SDL: 0.3381147540983606\n",
      "\n",
      "Model accuracy: 92.14285714285714\n"
     ]
    }
   ],
   "source": [
    "for key, value in metric_list[0].items():\n",
    "    if key == \"FOL\" or key ==\"FKL\" or key == \"BSC\" or key ==\"SDL\" or key ==\"Falls\":\n",
    "        specificity = mean([elem[key][\"Specificity\"] for elem in metric_list])\n",
    "        recall = mean([elem[key][\"Recall\"] for elem in metric_list])\n",
    "        precision = mean([elem[key][\"Precision\"] for elem in metric_list])\n",
    "        fScore = mean([elem[key][\"F-score\"] for elem in metric_list])\n",
    "        print(\"Specificity of \" + key + \": \" + str(specificity))\n",
    "        print(\"Recall of \" + key + \": \" + str(recall))\n",
    "        print(\"Precision of \" + key + \": \" + str(precision))\n",
    "        print(\"F1-score of \" + key + \": \" + str(fScore))\n",
    "        print(\"\")\n",
    "print(\"Model accuracy: \" + str(mean(model_accuracies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
    "def save_model(model):\n",
    "    torchscript_model = torch.jit.script(model)\n",
    "    torchscript_model_optimized = optimize_for_mobile(torchscript_model)\n",
    "    torch.jit.save(torchscript_model_optimized, \"MIS_RFD_model_quantized.pt\")\n",
    "    #torchscript_model_optimized._save_for_lite_interpreter(\"MIS_RFD_model_quantized.ptl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e29914317381cb5b054684ec136cf5f9d23d3fafddd70669a3da3e14954aacc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
